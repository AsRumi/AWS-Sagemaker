{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e9793a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3 # Used for reading S3 buckets if public access is given for that bucket.\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.session import s3_input, Session # To create a Session of Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a231603",
   "metadata": {},
   "source": [
    "### Creating an S3 bucket using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "182862c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap-south-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'sagemaker-ap-south-1-043309360841' # Has to be unique\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region) # Making sure I am currently running in Acia Pacific South region (Mumbai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0149dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The ap_south_1 location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == \"ap-south-1\":\n",
    "        s3.create_bucket(Bucket = bucket_name, CreateBucketConfiguration={'LocationConstraint': 'ap_south_1'})\n",
    "    print('S3 Bucket Created Successfully!')\n",
    "except Exception as e:\n",
    "    print(f\"S3 error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33886306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-south-1-043309360841/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a36e80",
   "metadata": {},
   "source": [
    "### Downloading and uploading the data into S3 Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c742ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded the dataset!\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print(\"Successfully downloaded the dataset!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")\n",
    "    \n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv', index_col = 0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print(f'Failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01e669",
   "metadata": {},
   "source": [
    "Data has been downloaded, now we split it for training and testing before we insert it into the S3 bucket. We are not separating the input from the output, rather entirely dumping the training data and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "188ac662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30891, 61) (10297, 61)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = np.split(model_data.sample(frac = 1, random_state = 402), [int(0.75 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89c500",
   "metadata": {},
   "source": [
    "The format of data in Amazon Sagemaker is different from the usual format which is used for training in the sense that the dependent feature appears as the first column and all the independent features follow afterwards. Therefore we need to tune the dataset to push the dependent features (ones that will be predicted by the model) to the front of the dataset and allow all other independent features to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f9799e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'],\n",
    "                                               axis = 1)],\n",
    "         axis = 1).to_csv('train.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601f5bf",
   "metadata": {},
   "source": [
    "Saving training data into the bucket and recalling it from the bucket for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce06d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data = 's3://{}/{}/train'.format(bucket_name, prefix), content_type = 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ba5c5",
   "metadata": {},
   "source": [
    "Now we do the same for the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d19a3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'],\n",
    "                                               axis = 1)],\n",
    "         axis = 1).to_csv('test.csv', index = False, header = False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data = 's3://{}/{}/test'.format(bucket_name, prefix), content_type = 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fa4a1",
   "metadata": {},
   "source": [
    "### Using an inbuilt XGBoost algorithm to perform the classification task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213cb24",
   "metadata": {},
   "source": [
    "Whenever you want to use an inbuilt algorithm from Sagemaker, you need to pull instances of those algorithms into your runtime. These algorithms are usually an image or in a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0325abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = retrieve(framework = 'xgboost',\n",
    "                    region = boto3.Session().region_name,\n",
    "                    version = '1.7-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1316042",
   "metadata": {},
   "source": [
    "You also need to initialize the hyperparameters if you want to save running time on Sagemaker, this step might cost you money since training on Sagemaker requires credits. However, the tutorial you were following provided you with these optimized hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a25f3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490196a",
   "metadata": {},
   "source": [
    "We create an estimator to finally compile everything we have done so far into one single step that can be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2dc0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(image_uri=container, # This pulls up the container we have mentioned that needs to be used for training, here, XGBoost Algorithm.\n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(), # This is the session IAM role, which provides details on the project and the buckets that can be used.\n",
    "                                          instance_count=1, # Number of Amazon EC2 instances that need to be used for training.\n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB, size that might be needed to store the output models (I think) \n",
    "                                          output_path=output_path,\n",
    "                                          # The following three arguments reduce the cost of running the training.\n",
    "                                          use_spot_instances=True, \n",
    "                                          max_run=300,\n",
    "                                          max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a836e14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-21-14-53-06-475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-21 14:53:06 Starting - Starting the training job...\n",
      "2024-10-21 14:53:26 Starting - Preparing the instances for training...\n",
      "2024-10-21 14:54:03 Downloading - Downloading the training image......\n",
      "2024-10-21 14:55:09 Training - Training image download completed. Training in progress....\n",
      "2024-10-21 14:55:30 Uploading - Uploading generated training model\u001b[34m[2024-10-21 14:55:25.719 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:25.742 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-2174857394255680867\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] creating symlink between Path /opt/ml/input/data/validation/test.csv and destination /tmp/sagemaker_xgboost_input_data/test.csv8989294355006334016\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Train matrix has 30891 rows and 59 columns\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Validation matrix has 10297 rows\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.274 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.275 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.275 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.275 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-10-21:14:55:26:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.57407#011validation-logloss:0.57381\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.386 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-10-21 14:55:26.389 ip-10-0-66-127.ap-south-1.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.49403#011validation-logloss:0.49379\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.43865#011validation-logloss:0.43837\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.39856#011validation-logloss:0.39855\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.36919#011validation-logloss:0.36963\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.34787#011validation-logloss:0.34843\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.33211#011validation-logloss:0.33285\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.32055#011validation-logloss:0.32139\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.31180#011validation-logloss:0.31290\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.30518#011validation-logloss:0.30648\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.30023#011validation-logloss:0.30136\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.29656#011validation-logloss:0.29808\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.29377#011validation-logloss:0.29542\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.29107#011validation-logloss:0.29306\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.28917#011validation-logloss:0.29127\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.28773#011validation-logloss:0.29024\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.28648#011validation-logloss:0.28957\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.28539#011validation-logloss:0.28881\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.28480#011validation-logloss:0.28841\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.28423#011validation-logloss:0.28780\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.28344#011validation-logloss:0.28762\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.28246#011validation-logloss:0.28683\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.28203#011validation-logloss:0.28671\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.28166#011validation-logloss:0.28661\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.28131#011validation-logloss:0.28653\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.28087#011validation-logloss:0.28627\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.28046#011validation-logloss:0.28611\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.27966#011validation-logloss:0.28572\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.27945#011validation-logloss:0.28548\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.27925#011validation-logloss:0.28547\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.27906#011validation-logloss:0.28538\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.27898#011validation-logloss:0.28540\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.27858#011validation-logloss:0.28521\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.27832#011validation-logloss:0.28518\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.27807#011validation-logloss:0.28519\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.27778#011validation-logloss:0.28510\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.27729#011validation-logloss:0.28492\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.27700#011validation-logloss:0.28486\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.27677#011validation-logloss:0.28490\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.27672#011validation-logloss:0.28485\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.27664#011validation-logloss:0.28481\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.27654#011validation-logloss:0.28489\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.27650#011validation-logloss:0.28500\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.27640#011validation-logloss:0.28504\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.27624#011validation-logloss:0.28510\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.27612#011validation-logloss:0.28516\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.27592#011validation-logloss:0.28531\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.27579#011validation-logloss:0.28541\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.27549#011validation-logloss:0.28548\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.27534#011validation-logloss:0.28538\u001b[0m\n",
      "\n",
      "2024-10-21 14:55:43 Completed - Training job completed\n",
      "Training seconds: 115\n",
      "Billable seconds: 40\n",
      "Managed Spot Training savings: 65.2%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test}) # s3_input_train and s3_input_test are the datasets in the bucket that we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00961e93",
   "metadata": {},
   "source": [
    "### Deploying trained model\n",
    "\n",
    "You deploy the model with the same Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8359e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-10-21-15-00-35-617\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-10-21-15-00-35-617\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-10-21-15-00-35-617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb4f1a",
   "metadata": {},
   "source": [
    "### Predicting using test data\n",
    "\n",
    "After deploying your model, it takes tabulated data in a serialized form and makes inferences on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "468cc2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10297,)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "import io\n",
    "\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')\n",
    "predictions_array = np.genfromtxt(io.StringIO(predictions[1:]), delimiter=',')\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "446d3143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0443828 , 0.04712602, 0.04159706, ..., 0.05219468, 0.03978461,\n",
       "       0.02386106])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f362f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.9%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (9017)    37% (141)\n",
      "Purchase        9% (894)     63% (245) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872843f",
   "metadata": {},
   "source": [
    "### Deleting endpoints and S3 Bucket data to avoid charges\n",
    "\n",
    "After finishing this tutorial, make sure you delete all your endpoints (estimator, buckets) to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c92352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2024-10-21-15-00-35-617\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"sagemaker-xgboost-2024-10-21-15-00-35-617\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m bucket_to_delete \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mresource(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mBucket(bucket_name)\n\u001b[1;32m      3\u001b[0m bucket_to_delete\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mall()\u001b[38;5;241m.\u001b[39mdelete()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4784\u001b[0m, in \u001b[0;36mSession.delete_endpoint\u001b[0;34m(self, endpoint_name)\u001b[0m\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Delete an Amazon SageMaker ``Endpoint``.\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \n\u001b[1;32m   4780\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   4781\u001b[0m \u001b[38;5;124;03m    endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to delete.\u001b[39;00m\n\u001b[1;32m   4782\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4783\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting endpoint with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, endpoint_name)\n\u001b[0;32m-> 4784\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"sagemaker-xgboost-2024-10-21-15-00-35-617\"."
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071067a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
